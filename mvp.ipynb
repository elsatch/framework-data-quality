{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import ckanapi\n",
    "import geopandas as gpd\n",
    "import nltk\n",
    "import petk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from shapely.geometry import mapping, shape\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from time import sleep\n",
    "\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE_FRAMEWORK = 'catalogue-quality-scores'\n",
    "\n",
    "RESOURCE_MODEL = 'scoring-methods'\n",
    "RESOURCE_SCORES = 'catalogue-scorecard'\n",
    "\n",
    "MODEL_VERSION = 'v0.0.3'\n",
    "\n",
    "# DIMENSIONS = ['interpretability', 'usability', 'metadata', 'freshness', 'granularity', 'completeness', 'accessibility'] # Ordered by importance\n",
    "DIMENSIONS = ['usability', 'metadata', 'freshness', 'completeness', 'accessibility'] # Ordered by importance\n",
    "\n",
    "DATA_CKAN = {\n",
    "    'address': 'https://ckan0.cf.opendata.inter.prod-toronto.ca'\n",
    "#     'apikey': ''\n",
    "}\n",
    "\n",
    "STORAGE_CKAN = {\n",
    "    'address': 'https://ckanadmin0.intra.dev-toronto.ca',\n",
    "    'apikey': '784f11cc-b170-4377-83a3-38ba28662b16'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(ckan, pid=PACKAGE_FRAMEWORK):\n",
    "    try:\n",
    "        model = ckan.action.package_show(id=pid)\n",
    "    except ckanapi.NotAuthorized:\n",
    "        raise Exception('Permission required to search for the framework package')\n",
    "    except ckanapi.NotFound:\n",
    "        raise Exception('Framework package not found')\n",
    "    \n",
    "    return {\n",
    "        r['name']: r for r in model.pop('resources')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_datastore(ckan, rid, rows=10000):\n",
    "    records = []\n",
    "    \n",
    "    is_geospatial = False\n",
    "    \n",
    "    has_more = True\n",
    "    while has_more:\n",
    "        result = ckan.action.datastore_search(id=rid, limit=rows, offset=len(records))\n",
    "        \n",
    "        records += result['records']\n",
    "        has_more = len(records) < result['total']\n",
    "    \n",
    "    df = pd.DataFrame(records).drop('_id', axis=1)\n",
    "    \n",
    "    if 'geometry' in df.columns:\n",
    "        df['geometry'] = df['geometry'].apply(lambda x: shape(json.loads(x)))\n",
    "        \n",
    "        df = gpd.GeoDataFrame(df, crs={'init': 'epsg:4326'})\n",
    "    \n",
    "    return df, [x for x in result['fields'] if x['id'] != '_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_usability(columns, data):\n",
    "    '''\n",
    "        How easy is it to use the data given how it is organized/structured?\n",
    "        \n",
    "        TODO's: \n",
    "            * level of nested fields?\n",
    "            * long vs. wide?\n",
    "            * if ID columns given, are these ID's common across datasets?\n",
    "    '''\n",
    "    \n",
    "    def parse_col_name(s):\n",
    "        camel_to_snake = re.sub(\n",
    "            '([a-z0-9])([A-Z])', \n",
    "            r'\\1_\\2', \n",
    "            re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', s)\n",
    "        ).lower()\n",
    "\n",
    "        return camel_to_snake == s, re.split('-|_|\\s', camel_to_snake)\n",
    "\n",
    "    metrics = {\n",
    "        'col_names': 0, # Are the column names easy to understand?\n",
    "        'col_constant': 0 # Are there columns where all values are constant?\n",
    "    }\n",
    "    \n",
    "    for f in columns:\n",
    "        is_camel, words = parse_col_name(f['id'])\n",
    "        eng_words = [ wordnet.synsets(w) for w in words if len(w) ]\n",
    "\n",
    "        if len(eng_words) / len(words) > 0.8:\n",
    "            metrics['col_names'] += (1 if not is_camel else 0.5) / len(columns)\n",
    "        \n",
    "        if not f['id'] == 'geometry' and data[f['id']].nunique() == 1:\n",
    "            metrics['col_constant'] += 1 / len(columns)\n",
    "    \n",
    "    if isinstance(data, gpd.GeoDataFrame):\n",
    "        counts = data['geometry'].is_valid.value_counts()\n",
    "        \n",
    "        metric['geo_validity'] = 1 - (counts[False] / (len(data) * 0.05)) if False in counts else 1\n",
    "    \n",
    "    return np.mean(list(metrics.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_FIELDS = ['collection_method', 'limitations', 'civic_issues', 'topics', 'owner_division', 'owner_email']\n",
    "\n",
    "def score_metadata(package, columns):\n",
    "    '''\n",
    "        How easy is it to understand the context of the data?\n",
    "        \n",
    "        TODO's: \n",
    "            * Measure the quality of the metadata as well\n",
    "    '''\n",
    "    \n",
    "    metrics = {\n",
    "        'desc_dataset': 0, # Does the metadata describe the dataset well?\n",
    "        'desc_columns': 0 # Does the metadata describe the data well?\n",
    "    }\n",
    "    \n",
    "    for field in METADATA_FIELDS:\n",
    "        if field in package and not package[field] is None and len(package[field]):\n",
    "            metrics['desc_dataset'] += 1 / len(METADATA_FIELDS)\n",
    "            \n",
    "    for f in columns:\n",
    "        if 'info' in f and len(f['info']['notes']):\n",
    "            metrics['desc_columns'] += 1 / len(columns)\n",
    "\n",
    "    return np.mean(list(metrics.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_MAP = {\n",
    "    'read-time': 1,\n",
    "    'daily': 1,\n",
    "    'weekly': 7,\n",
    "    'monthly': 30,\n",
    "    'quarterly': 52 * 7 / 4,\n",
    "    'semi-annually': 52 * 7 / 2,\n",
    "    'annually': 365\n",
    "}\n",
    "\n",
    "def score_freshness(package):\n",
    "    rr = package['refresh_rate'].lower()\n",
    "    \n",
    "    if rr in TIME_MAP and 'last_refreshed' in package and package['last_refreshed']: \n",
    "        elapsed = (dt.now() - dt.strptime(package['last_refreshed'], '%Y-%m-%dT%H:%M:%S.%f')).days / TIME_MAP[rr]\n",
    "        \n",
    "        return max(0, 1 - elapsed * 0.5)\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(dimensions, method='rs'):\n",
    "    N = len(dimensions)\n",
    "    \n",
    "    if method == 'sr':\n",
    "        denom = np.array([ ((1 / (i + 1)) + ((N + 1 - (i + 1)) / N)) for i, x in enumerate(dimensions) ]).sum()\n",
    "        weights = [ ((1 / (i + 1)) + ((N + 1 - (i + 1)) / N)) / denom for i, x in enumerate(dimensions) ]\n",
    "    elif method == 'rs':\n",
    "        denom = np.array([ (N + 1 - (i + 1)) for i, x in enumerate(dimensions)]).sum()\n",
    "        weights = [ (N + 1 - (i + 1)) / denom for i, x in enumerate(dimensions) ]\n",
    "    elif method == 'rr':\n",
    "        denom = np.array([ 1 / (i + 1) for i, x in enumerate(dimensions) ]).sum()\n",
    "        weights = [ (1 / (i + 1)) / denom for i, x in enumerate(dimensions) ]\n",
    "    elif method == 're':\n",
    "        exp = 0.2\n",
    "        denom = np.array([ (N + 1 - (i + 1)) ** exp for i, x in enumerate(dimensions) ]).sum()\n",
    "        weights = [ (N + 1 - (i + 1)) ** exp / denom for i, x in enumerate(dimensions) ]\n",
    "    else:\n",
    "        raise Exception('Invalid weighting method provided')\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(ckan, model, storage):\n",
    "    if not RESOURCE_MODEL in storage:\n",
    "        r = requests.post(\n",
    "            '{0}/api/3/action/resource_create'.format(ckan.address),\n",
    "            data={\n",
    "                'package_id': PACKAGE_FRAMEWORK,\n",
    "                'name': RESOURCE_MODEL,\n",
    "                'format': 'json',\n",
    "                'is_preview': False\n",
    "            },\n",
    "            headers={\n",
    "                'Authorization': ckan.apikey\n",
    "            },\n",
    "            files={\n",
    "                'upload': ('{0}.json'.format(RESOURCE_MODEL), json.dumps({}))\n",
    "            }\n",
    "        )\n",
    "\n",
    "        storage[RESOURCE_MODEL] = json.loads(r.content)['result']\n",
    "\n",
    "    r = requests.get(\n",
    "        storage[RESOURCE_MODEL]['url'],\n",
    "        headers={\n",
    "            'Authorization': ckan.apikey\n",
    "        }\n",
    "    )\n",
    "\n",
    "    scoring_methods = json.loads(r.content)\n",
    "    scoring_methods[MODEL_VERSION] = model\n",
    "\n",
    "    r = requests.post(\n",
    "        '{0}/api/3/action/resource_patch'.format(ckan.address),\n",
    "        data={\n",
    "            'id': storage[RESOURCE_MODEL]['id']\n",
    "        },\n",
    "        headers={\n",
    "            'Authorization': ckan.apikey\n",
    "        },\n",
    "        files={\n",
    "            'upload': ('{0}.json'.format(RESOURCE_MODEL), json.dumps(scoring_methods))\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_score(ckan, data, weight, dimensions, storage):\n",
    "    df = pd.DataFrame(data).set_index(['package', 'resource'])\n",
    "\n",
    "    scores = pd.DataFrame([weight] * len(df.index))\n",
    "    scores.index = df.index\n",
    "    scores.columns = dimensions\n",
    "\n",
    "    scores = df.multiply(scores)\n",
    "\n",
    "    df['score'] = scores.sum(axis=1)\n",
    "    df['score_norm'] = MinMaxScaler().fit_transform(df[['score']])\n",
    "\n",
    "    df = df.groupby('package').mean()\n",
    "\n",
    "    df['grade'] = pd.cut(df['score'], bins=[-1, .3, .5, .8, 1], labels=['D','C','B','A'])\n",
    "    df['grade_norm'] = pd.cut(df['score_norm'], bins=[-1, .3, .5, .8, 1], labels=['D','C','B','A'])\n",
    "\n",
    "    df['recorded_at'] = dt.now().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    df['model'] = MODEL_VERSION\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    if not RESOURCE_SCORES in storage:\n",
    "        storage[RESOURCE_SCORES] = ckan.action.datastore_create(\n",
    "            resource={\n",
    "                'package_id': PACKAGE_FRAMEWORK,\n",
    "                'name': RESOURCE_SCORES,\n",
    "                'format': 'csv',\n",
    "                'is_preview': False\n",
    "            },\n",
    "            records=df.to_dict(orient='row')\n",
    "        )\n",
    "    else:\n",
    "        ckan.action.datastore_upsert(\n",
    "            method='insert',\n",
    "            resource_id=storage[RESOURCE_SCORES]['resource_id'],\n",
    "            records=df.to_dict(orient='row')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ckanapi.RemoteCKAN(**DATA_CKAN)\n",
    "ckan = ckanapi.RemoteCKAN(**STORAGE_CKAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = source.action.current_package_list_with_resources(limit=500)\n",
    "\n",
    "data = []\n",
    "for p in packages:\n",
    "    for r in p['resources']:\n",
    "        if not 'datastore_active' in r or not r['datastore_active']:\n",
    "            continue\n",
    "        \n",
    "        content, fields = read_datastore(source, r['id'])\n",
    "        \n",
    "        data.append({\n",
    "            'package': p['name'],\n",
    "            'resource': r['name'],\n",
    "#             'interpretability': 1,\n",
    "            'usability': score_usability(p, fields, content),\n",
    "            'metadata': score_metadata(p, fields),\n",
    "            'freshness': score_freshness(p),\n",
    "#             'granularity': 1,\n",
    "            'completeness': 1 - (np.sum(len(content) - content.count()) / np.prod(content.shape)),\n",
    "            'accessibility': 1 if r['extract_job'] else 0.5\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = calculate_weights(DIMENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data).set_index(['package', 'resource'])\n",
    "\n",
    "scores = pd.DataFrame([weights] * len(df.index))\n",
    "scores.index = df.index\n",
    "scores.columns = DIMENSIONS\n",
    "\n",
    "scores = df.multiply(scores)\n",
    "\n",
    "df['score'] = scores.sum(axis=1)\n",
    "df['score_norm'] = MinMaxScaler().fit_transform(df[['score']])\n",
    "\n",
    "df = df.groupby('package').mean()\n",
    "\n",
    "df['grade'] = pd.cut(df['score'], bins=[-1, .3, .5, .8, 1], labels=['D','C','B','A'])\n",
    "df['grade_norm'] = pd.cut(df['score_norm'], bins=[-1, .3, .5, .8, 1], labels=['D','C','B','A'])\n",
    "\n",
    "df['recorded_at'] = dt.now()\n",
    "df['model'] = MODEL_VERSION\n",
    "\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DIMENSIONS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fd493bf65c8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;34m'weight'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwgt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#             'metrics': DIMENSIONS[dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         } for i, (dim, wgt) in enumerate(zip(DIMENSIONS, weights))\n\u001b[0m\u001b[1;32m     13\u001b[0m     ]\n\u001b[1;32m     14\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DIMENSIONS' is not defined"
     ]
    }
   ],
   "source": [
    "model = {\n",
    "    'aggregation_methods': {\n",
    "        'metrics_to_dimension': 'avg',\n",
    "        'dimensions_to_score': 'sum_and_reciprocal'\n",
    "    },\n",
    "    'dimensions': [\n",
    "        {\n",
    "            'name': dim,\n",
    "            'rank': i + 1,\n",
    "            'weight': wgt,\n",
    "#             'metrics': DIMENSIONS[dim]\n",
    "        } for i, (dim, wgt) in enumerate(zip(DIMENSIONS, weights))\n",
    "    ]\n",
    "}\n",
    "\n",
    "update_model(ckan, model, storage)\n",
    "update_score(ckan, data, weights, DIMENSIONS, storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
