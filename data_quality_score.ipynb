{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Data Toronto: Data Quality Score (DQS)\n",
    "This is the code behind the DQS, so we can share how the score is calculated. The goal of the DQS is to geive users an idea of how good data is without having to view it and give publishers pointers for improving their data - and an incentive to create _better_ datasets, not just _more_ of them.\n",
    "\n",
    "> High quality data enables high-quality impact\n",
    "\n",
    "### Resources\n",
    "Although a high-level overview of the DQS is provided throughout the notebook, it is highly recommended to read the articles where we share in detail why the DQS was created and how (beyond the code):\n",
    "\n",
    "* <a href=\"https://medium.com/@careduz/towards-a-data-quality-score-in-open-data-part-1-525e59f729e9\">Towards a Data Quality Score in open data (part 1) - Why Open Data Toronto created a score to assess data quality and what it measures</a>\n",
    "* <a href=\"https://medium.com/@careduz/towards-a-data-quality-score-in-open-data-part-2-3f193eb9e21d\">Towards a Data Quality Score in open data (partÂ 2) - How we created the DQS: a walkthrough for organizations facing similar challenges</a>\n",
    "\n",
    "### Notes\n",
    "This is a working notebook - hence \"TODO\" comments throughout the code to serve as reminders for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "\n",
    "import ckanapi\n",
    "import geopandas as gpd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from shapely.geometry import shape\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scoring DQS dimensions\n",
    "This version of the DQS is calculated from the scores of 5 underlying dimensions, each with a weight towards the final score, which are in turn scored based on equal-weighted metrics we use for measuring the dataset's performance in that dimension.\n",
    "\n",
    "This section contains the functions for scoring the dimensions. Each function checks every data \"resource\" in an individual dataset (in CKAN these are called \"packages\", hence the terminology used in the code), and the dataset score is the average of the resource scores.\n",
    "\n",
    "For example: if a dataset has 2 CSV files, each is scored across the dimensions and assigned a DQS, then the dataset DQS is the average of the resource scores.\n",
    "\n",
    "### Usability\n",
    "Accounts for 38% of the DQS. It is composed of 3 metrics.\n",
    "\n",
    "#### 1. Meaningful column names: proportion of words in column name in English\n",
    "To determine this, we first split column names by \"\\_\", \" \", \"-\" and camelCase. For example:\n",
    "\n",
    "* bdngHeightMetres ==> [\"bdng\", \"height\", \"metres\"]\n",
    "* bdng_height_metres ==> [\"bdng\", \"height\", \"metres\"]\n",
    "* bdng height metres ==> [\"bdng\", \"height\", \"metres\"]\n",
    "\n",
    "Then we look at the portion of English words in the resulting array. In this case, 2/3 words are English which means the name gets a \"meaning score\" of 66%.\n",
    "\n",
    "If the meaning score is above a threshold, which we set to 80\\% for now, then the column counts is said to have a \"meaningful name\".\n",
    "\n",
    "This is done for all the columns in the dataset, so if 7 of 10 column are meaningful, the dataset scores 70\\% for this metric.\n",
    "\n",
    "#### 2. Valid geometries (geospatial only)\n",
    "\n",
    "This is an automatic check for ensuring the geometry is well-formed across 2 dimensions. A common issue of invalid geometries is the inability to visualize them or work with them - often requires cleanup, thus making them more difficult to use.\n",
    "\n",
    "#### 3. Proportion of columns with a single value (a constant)\n",
    "\n",
    "Checks for columns with a single value, which adds cognitive load to the user - since it's a constant, this could be captured elsewhere (e.g. metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_usability(columns, data):\n",
    "    '''\n",
    "        How easy is it to use the data given how it is organized/structured?\n",
    "        \n",
    "        TODO's: \n",
    "            * level of nested fields?\n",
    "            * long vs. wide?\n",
    "            * if ID columns given, are these ID's common across datasets?\n",
    "    '''\n",
    "    \n",
    "    def parse_col_name(s):\n",
    "        camel_to_snake = re.sub(\n",
    "            '([a-z0-9])([A-Z])', \n",
    "            r'\\1_\\2', \n",
    "            re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', s)\n",
    "        ).lower()\n",
    "\n",
    "        return camel_to_snake == s, [x for x in re.split('-|_|\\s', camel_to_snake) if len(x)]\n",
    "\n",
    "    metrics = {\n",
    "        'col_names': 0, # Are the column names easy to understand?\n",
    "        'col_constant': 1 # Are there columns where all values are constant?\n",
    "    }\n",
    "    \n",
    "    for f in columns:\n",
    "        is_camel, words = parse_col_name(f['id'])\n",
    "        eng_words = [ w for w in words if len(wordnet.synsets(w)) ]\n",
    "\n",
    "        if len(eng_words) / len(words) > 0.8:\n",
    "            metrics['col_names'] += (1 if not is_camel else 0.5) / len(columns)\n",
    "        \n",
    "        if not f['id'] == 'geometry' and data[f['id']].nunique() <= 1:\n",
    "            metrics['col_constant'] -= 1 / len(columns)\n",
    "    \n",
    "    if isinstance(data, gpd.GeoDataFrame):\n",
    "        counts = data['geometry'].is_valid.value_counts()\n",
    "        \n",
    "        metrics['geo_validity'] = 1 - (counts[False] / (len(data) * 0.05)) if False in counts else 1\n",
    "    \n",
    "    return np.mean(list(metrics.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "Accounts for 25% of total weight. Ideally would measure whether the metadata is provided _and_ also how well described it is - this MVP version of the DQS focuses only on the former, whether the metadata exists, because the latter requires much more involvement... namely NLP and a training set. Hopefully that will follow in the future.\n",
    "\n",
    "#### Metadata fields checked\n",
    "Not all metadata fields are checked because many are required for publication, meaning they will always be present. Fields checked are:\n",
    "* Collection method\n",
    "* Limitations\n",
    "* Topics\n",
    "* Owner Email - _opendata@toronto.ca_ does not count because it's the default. Ideally, publishers will provide an email address (personal or distribution list) that gives users a direct connection to those who know the data.\n",
    "* Column Descriptions\n",
    "\n",
    "Fields get a value of 1 if there is metadata, and 0 if there isn't. The metric score is the mean across all fields - for instance: if 5 of 10 fields are filled, the dataset gets a 50% metadata score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_FIELDS = ['collection_method', 'limitations', 'topics', 'owner_email']\n",
    "\n",
    "def score_metadata(package, columns):\n",
    "    '''\n",
    "        How easy is it to understand the context of the data?\n",
    "        \n",
    "        TODO's: \n",
    "            * Measure the quality of the metadata as well\n",
    "    '''\n",
    "    \n",
    "    metrics = {\n",
    "        'desc_dataset': 0, # Does the metadata describe the dataset well?\n",
    "        'desc_columns': 0 # Does the metadata describe the data well?\n",
    "    }\n",
    "    \n",
    "    for field in METADATA_FIELDS:\n",
    "        if field in package and package[field] and not (field == 'owner_email' and 'opendata' in package[field]):\n",
    "            metrics['desc_dataset'] += 1 / len(METADATA_FIELDS)\n",
    "            \n",
    "    for f in columns:\n",
    "        if 'info' in f and len(f['info']['notes']) and f['info']['notes'].strip() != f['id']:\n",
    "            metrics['desc_columns'] += 1 / len(columns)\n",
    "\n",
    "    return np.mean(list(metrics.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freshness\n",
    "Accounts for 18% of total weight. It is composed of 2 metrics, with equal weights towards the Freshness dimension score.\n",
    "\n",
    "**Note**: If dataset is real-time, automatically receives 100%... doesn't get any more fresh than that.\n",
    "\n",
    "#### 1. Duration between published refresh rate and when data was last refreshed\n",
    "Say a dataset is supposed to be refreshed _daily_ but has not been updated for 7 days. In this case, the dataset is 7 periods behind.\n",
    "\n",
    "Dataset scores 0 for this metric if more than 2 periods have elapsed since last refresh. Otherwise, the score drops gradually between 1 and periods missed.\n",
    "\n",
    "#### 2. Duration between today and date last refreshed\n",
    "We decided on this based on helpful feedback from the Data Quality Working Group and the community. It simply considers the time between the dataset was last refreshed and today - regardless of the periods. Metric starts decreasing after 6 months, and reaches 0 after 3 years. \n",
    "\n",
    "We accept this will put datasets with lengthy refresh rates, yearly or above (e.g. census), or datasets not refreshed at a disadvantage because the goal is to encourage more timely datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_MAP = {\n",
    "    'daily': 1,\n",
    "    'weekly': 7,\n",
    "    'monthly': 30,\n",
    "    'quarterly': 52 * 7 / 4,\n",
    "    'semi-annually': 52 * 7 / 2,\n",
    "    'annually': 365\n",
    "}\n",
    "\n",
    "def score_freshness(package):\n",
    "    '''\n",
    "        How up to date is the data?\n",
    "    '''\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    rr = package['refresh_rate'].lower()\n",
    "    \n",
    "    if rr == 'real-time':\n",
    "        return 1\n",
    "    elif rr in TIME_MAP and 'last_refreshed' in package and package['last_refreshed']: \n",
    "        days = (dt.utcnow() - dt.strptime(package['last_refreshed'], '%Y-%m-%dT%H:%M:%S.%f')).days\n",
    "        \n",
    "        # Greater than 2 periods have a score of 0\n",
    "        metrics['elapse_periods'] = max(0, 1 - math.floor(days / TIME_MAP[rr]) / 2)\n",
    "        \n",
    "        # Decrease the score starting from ~0.5 years to ~3 years\n",
    "        metrics['elapse_days'] = 1 - (1 / (1 + np.exp(4 * (2.25 - days/365))))\n",
    "        \n",
    "        return np.mean(list(metrics.values()))\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness\n",
    "Accounts for 12% of total weight. This is a simple one: % of cells with a value (1 - proportion of empty cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_completeness(data):\n",
    "    '''\n",
    "        How much of the data is missing?\n",
    "    '''\n",
    "    return 1 - (np.sum(len(data) - data.count()) / np.prod(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessibility\n",
    "Accounts for 7% of total weight. Although all datasets in scope are available via API (since they are all in the datastore), they are differentiated in this metric:\n",
    "\n",
    "1. Datasets received as flat CSV files (which are pushed to the datastore automatically) get 50%\n",
    "2. Datasets automatically pushed to the datastore on a schedule (which is captured in \"extract_job\" resource field) get 100%\n",
    "\n",
    "This is to encourage publishers to increasingly adopt automated refresh methods while, at the same time, recognizing those who provide data in an open, machine-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_accessibility(resource):\n",
    "    '''\n",
    "        Is data available via APIs?\n",
    "    '''\n",
    "    return 1 if 'extract_job' in resource and resource['extract_job'] else 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculating dimension weights\n",
    "Dimension weights are calculated based on rank weighting methods - after trying several, we decided on \"SR\" (Sum Reciprocal). This function takes the array of dimensions and returns the weights.\n",
    "\n",
    "To learn more about the weighting method, refer to the resources articles shared at the beginning of this notebook (Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(dimensions, method='sr'):\n",
    "    N = len(dimensions)\n",
    "    \n",
    "    if method == 'sr':\n",
    "        denom = np.array([ ((1 / (i + 1)) + ((N + 1 - (i + 1)) / N)) for i, x in enumerate(dimensions) ]).sum()\n",
    "        weights = [ ((1 / (i + 1)) + ((N + 1 - (i + 1)) / N)) / denom for i, x in enumerate(dimensions) ]\n",
    "    elif method == 'rs':\n",
    "        denom = np.array([ (N + 1 - (i + 1)) for i, x in enumerate(dimensions)]).sum()\n",
    "        weights = [ (N + 1 - (i + 1)) / denom for i, x in enumerate(dimensions) ]\n",
    "    elif method == 'rr':\n",
    "        denom = np.array([ 1 / (i + 1) for i, x in enumerate(dimensions) ]).sum()\n",
    "        weights = [ (1 / (i + 1)) / denom for i, x in enumerate(dimensions) ]\n",
    "    elif method == 're':\n",
    "        exp = 0.2\n",
    "        denom = np.array([ (N + 1 - (i + 1)) ** exp for i, x in enumerate(dimensions) ]).sum()\n",
    "        weights = [ (N + 1 - (i + 1)) ** exp / denom for i, x in enumerate(dimensions) ]\n",
    "    else:\n",
    "        raise Exception('Invalid weighting method provided')\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility functions for scoring in CKAN\n",
    "\n",
    "\n",
    "#### Reading resource from CKAN datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_datastore(ckan, rid, rows=10000):\n",
    "    records = []\n",
    "    \n",
    "    is_geospatial = False\n",
    "    \n",
    "    has_more = True\n",
    "    while has_more:\n",
    "        result = ckan.action.datastore_search(id=rid, limit=rows, offset=len(records))\n",
    "        \n",
    "        records += result['records']\n",
    "        has_more = len(records) < result['total']\n",
    "    \n",
    "    df = pd.DataFrame(records).drop('_id', axis=1)\n",
    "    \n",
    "    if 'geometry' in df.columns:\n",
    "        df['geometry'] = df['geometry'].apply(lambda x: shape(json.loads(x)))\n",
    "        \n",
    "        df = gpd.GeoDataFrame(df, crs={'init': 'epsg:4326'})\n",
    "    \n",
    "    return df, [x for x in result['fields'] if x['id'] != '_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving DQS framework from Open Data Toronto CKAN\n",
    "This contains the package, and resources, for the DQS in our portal. We will need this to write the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE_DQS = 'catalogue-quality-scores'\n",
    "\n",
    "def get_framework(ckan, pid=PACKAGE_DQS):\n",
    "    try:\n",
    "        framework = ckan.action.package_show(id=pid)\n",
    "    except ckanapi.NotAuthorized:\n",
    "        raise Exception('Permission required to search for the framework package')\n",
    "    except ckanapi.NotFound:\n",
    "        raise Exception('Framework package not found')\n",
    "    \n",
    "    return {\n",
    "        r['name']: r for r in framework.pop('resources')\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an HTTP Response (placeholder - not yet in use)\n",
    "Will use this when moving the function to a serverless execution platform such as AWS Lambda or GCP Cloud Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_response(code, message=''):\n",
    "    response = {\n",
    "        'statusCode': code,\n",
    "        'headers': {\n",
    "            'Access-Control-Allow-Origin': '*',\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if message:\n",
    "        response['body'] = message\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bringing it all together: scoring the catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCE_MODEL = 'scoring-models'\n",
    "MODEL_VERSION = 'v0.1.0'\n",
    "\n",
    "RESOURCE_SCORES = 'catalogue-scorecard'\n",
    "\n",
    "DIMENSIONS = ['usability', 'metadata', 'freshness', 'completeness', 'accessibility'] # Ranked in order\n",
    "\n",
    "BINS = {\n",
    "    'Bronze': 0.6,\n",
    "    'Silver': 0.8,\n",
    "    'Gold': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_catalogue(event={}, context={}):\n",
    "    ckan = ckanapi.RemoteCKAN(**{\n",
    "        'address': '', # CKAN URL here\n",
    "        # 'apikey' here to write back to CKAN\n",
    "    })\n",
    "    \n",
    "    weights = calculate_weights(DIMENSIONS)\n",
    "    fw = {\n",
    "        'aggregation_methods': {\n",
    "            'metrics_to_dimension': 'avg',\n",
    "            'dimensions_to_score': 'sum_and_reciprocal'\n",
    "        },\n",
    "        'dimensions': [\n",
    "            {\n",
    "                'name': dim,\n",
    "                'rank': i + 1,\n",
    "                'weights': wgt,\n",
    "            } for i, (dim, wgt) in enumerate(zip(DIMENSIONS, weights))\n",
    "        ],\n",
    "        'bins': BINS\n",
    "    }\n",
    "    \n",
    "    packages = ckan.action.current_package_list_with_resources(limit=500)\n",
    "\n",
    "    storage = get_framework(ckan)\n",
    "\n",
    "    data = []\n",
    "    for p in tqdm(packages, 'Datasets Scored'):\n",
    "        for r in p['resources']:\n",
    "            if not 'datastore_active' in r or not r['datastore_active']:\n",
    "                continue\n",
    "\n",
    "            content, fields = read_datastore(ckan, r['id'])\n",
    "\n",
    "            data.append({\n",
    "                'package': p['name'],\n",
    "                'resource': r['name'],\n",
    "                'usability': score_usability(fields, content),\n",
    "                'metadata': score_metadata(p, fields),\n",
    "                'freshness': score_freshness(p),\n",
    "                'completeness': score_completeness(content),\n",
    "                'accessibility': score_accessibility(r)\n",
    "            })\n",
    "    \n",
    "    # create JSON resource to capture scoring model details, if not already in storage\n",
    "    if not RESOURCE_MODEL in storage and ckan.apikey:\n",
    "        r = requests.post(\n",
    "            '{0}/api/3/action/resource_create'.format(ckan.address),\n",
    "            data={\n",
    "                'package_id': PACKAGE_DQS,\n",
    "                'name': RESOURCE_MODEL,\n",
    "                'format': 'json',\n",
    "                'is_preview': False,\n",
    "                'is_zipped': False\n",
    "            },\n",
    "            headers={\n",
    "                'Authorization': ckan.apikey\n",
    "            },\n",
    "            files={\n",
    "                'upload': ('{0}.json'.format(RESOURCE_MODEL), json.dumps({}))\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        storage[RESOURCE_MODEL] = json.loads(r.content)['result']\n",
    "\n",
    "    r = requests.get(\n",
    "        storage[RESOURCE_MODEL]['url'],\n",
    "        headers={\n",
    "            'Authorization': ckan.apikey\n",
    "        }\n",
    "    )\n",
    "\n",
    "    scoring_methods = json.loads(r.content)\n",
    "    scoring_methods[MODEL_VERSION] = fw\n",
    "\n",
    "    r = requests.post(\n",
    "        '{0}/api/3/action/resource_patch'.format(ckan.address),\n",
    "        data={\n",
    "            'id': storage[RESOURCE_MODEL]['id']\n",
    "        },\n",
    "        headers={\n",
    "            'Authorization': ckan.apikey\n",
    "        },\n",
    "        files={\n",
    "            'upload': ('{0}.json'.format(RESOURCE_MODEL), json.dumps(scoring_methods))\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    df = pd.DataFrame(data).set_index(['package', 'resource'])\n",
    "\n",
    "    scores = pd.DataFrame([weights] * len(df.index))\n",
    "    scores.index = df.index\n",
    "    scores.columns = DIMENSIONS\n",
    "\n",
    "    scores = df.multiply(scores)\n",
    "\n",
    "    df['score'] = scores.sum(axis=1)\n",
    "    df['score_norm'] = MinMaxScaler().fit_transform(df[['score']])\n",
    "\n",
    "    df = df.groupby('package').mean()\n",
    "\n",
    "    \n",
    "    labels = list(BINS.keys())\n",
    "    \n",
    "    bins = [-1]\n",
    "    bins.extend(BINS.values())\n",
    "    \n",
    "    df['grade'] = pd.cut(df['score_norm'], bins=bins, labels=labels)\n",
    "    df['grade_norm'] = pd.cut(df['score_norm'], bins=bins, labels=labels)\n",
    "\n",
    "    df['recorded_at'] = dt.now().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    df['version'] = MODEL_VERSION\n",
    "\n",
    "    df = df.reset_index()\n",
    "    df = df.round(2)\n",
    "    \n",
    "    if not ckan.apikey:\n",
    "        return df\n",
    "    \n",
    "    if not RESOURCE_SCORES in storage:\n",
    "        storage[RESOURCE_SCORES] = ckan.action.datastore_create(\n",
    "            resource={\n",
    "                'package_id': PACKAGE_DQS,\n",
    "                'name': RESOURCE_SCORES,\n",
    "                'format': 'csv',\n",
    "                'is_preview': True,\n",
    "                'is_zipped': True\n",
    "            },\n",
    "            fields=[ { 'id': x} for x in df.columns.values ],\n",
    "            records=df.to_dict(orient='row')\n",
    "        )\n",
    "    else:\n",
    "        ckan.action.datastore_upsert(\n",
    "            method='insert',\n",
    "            resource_id=storage[RESOURCE_SCORES]['id'],\n",
    "            records=df.to_dict(orient='row')\n",
    "        )\n",
    "    \n",
    "    # build_response(200, message='')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets Scored: 100%|ââââââââââ| 326/326 [03:27<00:00,  1.57it/s]  \n"
     ]
    }
   ],
   "source": [
    "scores = score_catalogue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>package</th>\n",
       "      <th>usability</th>\n",
       "      <th>metadata</th>\n",
       "      <th>freshness</th>\n",
       "      <th>completeness</th>\n",
       "      <th>accessibility</th>\n",
       "      <th>score</th>\n",
       "      <th>score_norm</th>\n",
       "      <th>grade</th>\n",
       "      <th>grade_norm</th>\n",
       "      <th>recorded_at</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air-conditioned-and-cool-spaces-heat-relief-ne...</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.74</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Silver</td>\n",
       "      <td>2020-02-10T15:48:39Z</td>\n",
       "      <td>v0.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air-conditioned-public-places-cooling-centres</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.30</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>2020-02-10T15:48:39Z</td>\n",
       "      <td>v0.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>annual-summary-of-reportable-communicable-dise...</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.51</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>2020-02-10T15:48:39Z</td>\n",
       "      <td>v0.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apartment-building-evaluation</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.97</td>\n",
       "      <td>Gold</td>\n",
       "      <td>Gold</td>\n",
       "      <td>2020-02-10T15:48:39Z</td>\n",
       "      <td>v0.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apartment-building-registration</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.60</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Silver</td>\n",
       "      <td>2020-02-10T15:48:39Z</td>\n",
       "      <td>v0.1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             package  usability  metadata  \\\n",
       "0  air-conditioned-and-cool-spaces-heat-relief-ne...       0.86      0.84   \n",
       "1      air-conditioned-public-places-cooling-centres       0.85      0.25   \n",
       "2  annual-summary-of-reportable-communicable-dise...       0.69      0.25   \n",
       "3                      apartment-building-evaluation       0.94      0.75   \n",
       "4                    apartment-building-registration       0.59      0.69   \n",
       "\n",
       "   freshness  completeness  accessibility  score  score_norm   grade  \\\n",
       "0        0.5          0.69            1.0   0.78        0.74  Silver   \n",
       "1        0.0          1.00            0.5   0.54        0.30  Bronze   \n",
       "2        1.0          0.98            0.5   0.66        0.51  Bronze   \n",
       "3        1.0          0.96            1.0   0.91        0.97    Gold   \n",
       "4        1.0          0.49            1.0   0.71        0.60  Silver   \n",
       "\n",
       "  grade_norm           recorded_at version  \n",
       "0     Silver  2020-02-10T15:48:39Z  v0.1.0  \n",
       "1     Bronze  2020-02-10T15:48:39Z  v0.1.0  \n",
       "2     Bronze  2020-02-10T15:48:39Z  v0.1.0  \n",
       "3       Gold  2020-02-10T15:48:39Z  v0.1.0  \n",
       "4     Silver  2020-02-10T15:48:39Z  v0.1.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
